{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XP2x0pxJ9uJK"
      },
      "source": [
        "## **Notebook #10**\n",
        "## Solving a Maze with Deep Reinforcement Learning.\n",
        "**Professor:** Fernando J. Von Zuben <br>\n",
        "**Aluno(a):** Arthur Felipe dos Santos Fernandes\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hFi1P1DT4FgS"
      },
      "source": [
        "# Code based on [this content](https://www.samyzaf.com/ML/rl/qmaze.html)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rjhi5kdG4FgT"
      },
      "source": [
        "### Importações e definições"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-aDpGQDz4Fg0"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "from time import sleep\n",
        "from IPython import display\n",
        "import pylab as pl\n",
        "import os, sys, time, datetime, json, random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation\n",
        "from tensorflow.keras.optimizers import SGD , Adam, RMSprop\n",
        "from keras.layers import PReLU\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "JQs4V1wY_jJS"
      },
      "outputs": [],
      "source": [
        "visited_mark = 0.8  # Cells visited by the rat will be painted by gray 0.8\n",
        "rat_mark = 0.5      # The current rat cell will be painted by gray 0.5\n",
        "LEFT = 0\n",
        "UP = 1\n",
        "RIGHT = 2\n",
        "DOWN = 3\n",
        "\n",
        "# Actions dictionary\n",
        "actions_dict = {\n",
        "    LEFT: 'left',\n",
        "    UP: 'up',\n",
        "    RIGHT: 'right',\n",
        "    DOWN: 'down',\n",
        "}\n",
        "\n",
        "num_actions = len(actions_dict)\n",
        "\n",
        "# Exploration factor\n",
        "epsilon = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "d1gPgFfQAMu_"
      },
      "outputs": [],
      "source": [
        "# maze is a 2d Numpy array of floats between 0.0 to 1.0\n",
        "# 1.0 corresponds to a free cell, and 0.0 an occupied cell\n",
        "# rat = (row, col) initial rat position (defaults to (0,0))\n",
        "\n",
        "class Qmaze(object):\n",
        "    def __init__(self, maze, rat=(0,0)):\n",
        "        self._maze = np.array(maze)\n",
        "        nrows, ncols = self._maze.shape\n",
        "        self.target = (nrows-1, ncols-1)   # target cell where the \"cheese\" is\n",
        "        self.free_cells = [(r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0]\n",
        "        self.free_cells.remove(self.target)\n",
        "        if self._maze[self.target] == 0.0:\n",
        "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
        "        if not rat in self.free_cells:\n",
        "            raise Exception(\"Invalid Rat Location: must sit on a free cell\")\n",
        "        self.reset(rat)\n",
        "\n",
        "    def reset(self, rat):\n",
        "        self.rat = rat\n",
        "        self.maze = np.copy(self._maze)\n",
        "        nrows, ncols = self.maze.shape\n",
        "        row, col = rat\n",
        "        self.maze[row, col] = rat_mark\n",
        "        self.state = (row, col, 'start')\n",
        "        self.min_reward = -0.5 * self.maze.size\n",
        "        self.total_reward = 0\n",
        "        self.visited = set()\n",
        "\n",
        "    def update_state(self, action):\n",
        "        nrows, ncols = self.maze.shape\n",
        "        nrow, ncol, nmode = rat_row, rat_col, mode = self.state\n",
        "\n",
        "        if self.maze[rat_row, rat_col] > 0.0:\n",
        "            self.visited.add((rat_row, rat_col))  # mark visited cell\n",
        "\n",
        "        valid_actions = self.valid_actions()\n",
        "                \n",
        "        if not valid_actions:\n",
        "            nmode = 'blocked'\n",
        "        elif action in valid_actions:\n",
        "            nmode = 'valid'\n",
        "            if action == LEFT:\n",
        "                ncol -= 1\n",
        "            elif action == UP:\n",
        "                nrow -= 1\n",
        "            if action == RIGHT:\n",
        "                ncol += 1\n",
        "            elif action == DOWN:\n",
        "                nrow += 1\n",
        "        else:                  # invalid action, no change in rat position\n",
        "            mode = 'invalid'\n",
        "\n",
        "        # new state\n",
        "        self.state = (nrow, ncol, nmode)\n",
        "\n",
        "    def get_reward(self):\n",
        "        rat_row, rat_col, mode = self.state\n",
        "        nrows, ncols = self.maze.shape\n",
        "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
        "            return 1.0\n",
        "        if mode == 'blocked':\n",
        "            return self.min_reward - 1\n",
        "        if (rat_row, rat_col) in self.visited:\n",
        "            return -0.25\n",
        "        if mode == 'invalid':\n",
        "            return -0.75\n",
        "        if mode == 'valid':\n",
        "            return -0.04\n",
        "\n",
        "    def act(self, action):\n",
        "        self.update_state(action)\n",
        "        reward = self.get_reward()\n",
        "        self.total_reward += reward\n",
        "        status = self.game_status()\n",
        "        envstate = self.observe()\n",
        "        return envstate, reward, status\n",
        "\n",
        "    def observe(self):\n",
        "        canvas = self.draw_env()\n",
        "        envstate = canvas.reshape((1, -1))\n",
        "        return envstate\n",
        "\n",
        "    def draw_env(self):\n",
        "        canvas = np.copy(self.maze)\n",
        "        nrows, ncols = self.maze.shape\n",
        "        # clear all visual marks\n",
        "        for r in range(nrows):\n",
        "            for c in range(ncols):\n",
        "                if canvas[r,c] > 0.0:\n",
        "                    canvas[r,c] = 1.0\n",
        "        # draw the rat\n",
        "        row, col, valid = self.state\n",
        "        canvas[row, col] = rat_mark\n",
        "        return canvas\n",
        "\n",
        "    def game_status(self):\n",
        "        if self.total_reward < self.min_reward:\n",
        "            return 'lose'\n",
        "        rat_row, rat_col, mode = self.state\n",
        "        nrows, ncols = self.maze.shape\n",
        "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
        "            return 'win'\n",
        "\n",
        "        return 'not_over'\n",
        "\n",
        "    def valid_actions(self, cell=None):\n",
        "        if cell is None:\n",
        "            row, col, mode = self.state\n",
        "        else:\n",
        "            row, col = cell\n",
        "        actions = [0, 1, 2, 3]\n",
        "        nrows, ncols = self.maze.shape\n",
        "        if row == 0:\n",
        "            actions.remove(1)\n",
        "        elif row == nrows-1:\n",
        "            actions.remove(3)\n",
        "\n",
        "        if col == 0:\n",
        "            actions.remove(0)\n",
        "        elif col == ncols-1:\n",
        "            actions.remove(2)\n",
        "\n",
        "        if row>0 and self.maze[row-1,col] == 0.0:\n",
        "            actions.remove(1)\n",
        "        if row<nrows-1 and self.maze[row+1,col] == 0.0:\n",
        "            actions.remove(3)\n",
        "\n",
        "        if col>0 and self.maze[row,col-1] == 0.0:\n",
        "            actions.remove(0)\n",
        "        if col<ncols-1 and self.maze[row,col+1] == 0.0:\n",
        "            actions.remove(2)\n",
        "\n",
        "        return actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ZEgvv0QHAcqO"
      },
      "outputs": [],
      "source": [
        "def show(qmaze):\n",
        "    plt.grid('on')\n",
        "    nrows, ncols = qmaze.maze.shape\n",
        "    ax = plt.gca()\n",
        "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
        "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "    canvas = np.copy(qmaze.maze)\n",
        "    for row,col in qmaze.visited:\n",
        "        canvas[row,col] = 0.6\n",
        "    rat_row, rat_col, _ = qmaze.state\n",
        "    canvas[rat_row, rat_col] = 0.3   # rat cell\n",
        "    canvas[nrows-1, ncols-1] = 0.9 # cheese cell\n",
        "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "nXYrCEK_EZ-U"
      },
      "outputs": [],
      "source": [
        "# The next cell and this cell are mutually exclusive. Do not execute both.\n",
        "maze = np.array([\n",
        "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
        "    [ 1.,  0.,  1.,  1.,  1.,  0.,  1.,  1.],\n",
        "    [ 1.,  1.,  1.,  1.,  0.,  1.,  0.,  1.],\n",
        "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
        "    [ 1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
        "    [ 1.,  1.,  1.,  0.,  1.,  0.,  0.,  0.],\n",
        "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
        "    [ 1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5Qj1TKfnA9qu"
      },
      "outputs": [],
      "source": [
        "def play_game(model, qmaze, rat_cell):\n",
        "    qmaze.reset(rat_cell)\n",
        "    envstate = qmaze.observe()\n",
        "    while True:\n",
        "        prev_envstate = envstate\n",
        "        # get next action\n",
        "        q = model.predict(prev_envstate)\n",
        "        action = np.argmax(q[0])\n",
        "\n",
        "        # apply action, get rewards and new state\n",
        "        envstate, reward, game_status = qmaze.act(action)\n",
        "        if game_status == 'win':\n",
        "            return True\n",
        "        elif game_status == 'lose':\n",
        "            return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "zw53DdHQBKF0"
      },
      "outputs": [],
      "source": [
        "def completion_check(model, qmaze):\n",
        "    for cell in qmaze.free_cells:\n",
        "        if not qmaze.valid_actions(cell):\n",
        "            return False\n",
        "        if not play_game(model, qmaze, cell):\n",
        "            return False\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Bm9WstgwBgjg"
      },
      "outputs": [],
      "source": [
        "class Experience(object):\n",
        "    def __init__(self, model, max_memory=100, discount=0.95):\n",
        "        self.model = model\n",
        "        self.max_memory = max_memory\n",
        "        self.discount = discount\n",
        "        self.memory = list()\n",
        "        self.num_actions = model.output_shape[-1]\n",
        "\n",
        "    def remember(self, episode):\n",
        "        # episode = [envstate, action, reward, envstate_next, game_over]\n",
        "        # memory[i] = episode\n",
        "        # envstate == flattened 1d maze cells info, including rat cell (see method: observe)\n",
        "        self.memory.append(episode)\n",
        "        if len(self.memory) > self.max_memory:\n",
        "            del self.memory[0]\n",
        "\n",
        "    def predict(self, envstate):\n",
        "        return self.model.predict(envstate, verbose = 0)[0]\n",
        "\n",
        "    def get_data(self, data_size=10):\n",
        "        env_size = self.memory[0][0].shape[1]   # envstate 1d size (1st element of episode)\n",
        "        mem_size = len(self.memory)\n",
        "        data_size = min(mem_size, data_size)\n",
        "        inputs = np.zeros((data_size, env_size))\n",
        "        targets = np.zeros((data_size, self.num_actions))\n",
        "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
        "            envstate, action, reward, envstate_next, game_over = self.memory[j]\n",
        "            inputs[i] = envstate\n",
        "            # There should be no target values for actions not taken.\n",
        "            targets[i] = self.predict(envstate)\n",
        "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
        "            Q_sa = np.max(self.predict(envstate_next))\n",
        "            if game_over:\n",
        "                targets[i, action] = reward\n",
        "            else:\n",
        "                # reward + gamma * max_a' Q(s', a')\n",
        "                targets[i, action] = reward + self.discount * Q_sa\n",
        "        return inputs, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-QvK92JaBiL7"
      },
      "outputs": [],
      "source": [
        "def qtrain(model, maze, **opt):\n",
        "    global epsilon\n",
        "    n_epoch = opt.get('n_epoch', 15000)\n",
        "    max_memory = opt.get('max_memory', 1000)\n",
        "    data_size = opt.get('data_size', 50)\n",
        "    weights_file = opt.get('weights_file', \"\")\n",
        "    name = opt.get('name', 'model')\n",
        "    start_time = datetime.datetime.now()\n",
        "\n",
        "    # If you want to continue training from a previous model,\n",
        "    # just supply the h5 file name to weights_file option\n",
        "    if weights_file:\n",
        "        print(\"loading weights from file: %s\" % (weights_file,))\n",
        "        model.load_weights(weights_file)\n",
        "\n",
        "    # Construct environment/game from numpy array: maze (see above)\n",
        "    qmaze = Qmaze(maze)\n",
        "\n",
        "    # Initialize experience replay object\n",
        "    experience = Experience(model, max_memory=max_memory)\n",
        "\n",
        "    win_history = []   # history of win/lose game\n",
        "    n_free_cells = len(qmaze.free_cells)\n",
        "    hsize = qmaze.maze.size//2   # history window size\n",
        "    win_rate = 0.0\n",
        "    imctr = 1\n",
        "\n",
        "    for epoch in range(n_epoch):\n",
        "        loss = 0.0\n",
        "        rat_cell = random.choice(qmaze.free_cells)\n",
        "        qmaze.reset(rat_cell)\n",
        "        game_over = False\n",
        "\n",
        "        # get initial envstate (1d flattened canvas)\n",
        "        envstate = qmaze.observe()\n",
        "\n",
        "        n_episodes = 0\n",
        "        while not game_over:\n",
        "            valid_actions = qmaze.valid_actions()\n",
        "            if not valid_actions: break\n",
        "            prev_envstate = envstate\n",
        "            # Get next action\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = random.choice(valid_actions)\n",
        "            else:\n",
        "                action = np.argmax(experience.predict(prev_envstate))\n",
        "\n",
        "            # Apply action, get reward and new envstate\n",
        "            envstate, reward, game_status = qmaze.act(action)\n",
        "            if game_status == 'win':\n",
        "                win_history.append(1)\n",
        "                game_over = True\n",
        "            elif game_status == 'lose':\n",
        "                win_history.append(0)\n",
        "                game_over = True\n",
        "            else:\n",
        "                game_over = False\n",
        "\n",
        "            # Store episode (experience)\n",
        "            episode = [prev_envstate, action, reward, envstate, game_over]\n",
        "            experience.remember(episode)\n",
        "            n_episodes += 1\n",
        "\n",
        "            # Train neural network model\n",
        "            inputs, targets = experience.get_data(data_size=data_size)\n",
        "            h = model.fit(\n",
        "                inputs,\n",
        "                targets,\n",
        "                epochs=8,\n",
        "                batch_size=16,\n",
        "                verbose=0,\n",
        "            )\n",
        "            loss = model.evaluate(inputs, targets, verbose=0)\n",
        "\n",
        "        if len(win_history) > hsize:\n",
        "            win_rate = sum(win_history[-hsize:]) / hsize\n",
        "    \n",
        "        dt = datetime.datetime.now() - start_time\n",
        "        t = format_time(dt.total_seconds())\n",
        "        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
        "        print(template.format(epoch, n_epoch-1, loss, n_episodes, sum(win_history), win_rate, t))\n",
        "        # we simply check if training has exhausted all free cells and if in all\n",
        "        # cases the agent won\n",
        "        if win_rate > 0.9 : epsilon = 0.05\n",
        "        if sum(win_history[-hsize:]) == hsize and completion_check(model, qmaze):\n",
        "            print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
        "            break\n",
        "\n",
        "    # Save trained model weights and architecture, this will be used by the visualization code\n",
        "    h5file = name + \".h5\"\n",
        "    json_file = name + \".json\"\n",
        "    model.save_weights(h5file, overwrite=True)\n",
        "    with open(json_file, \"w\") as outfile:\n",
        "        json.dump(model.to_json(), outfile)\n",
        "    end_time = datetime.datetime.now()\n",
        "    dt = datetime.datetime.now() - start_time\n",
        "    seconds = dt.total_seconds()\n",
        "    t = format_time(seconds)\n",
        "    print('files: %s, %s' % (h5file, json_file))\n",
        "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
        "    return seconds\n",
        "\n",
        "# This is a small utility for printing readable time strings:\n",
        "def format_time(seconds):\n",
        "    if seconds < 400:\n",
        "        s = float(seconds)\n",
        "        return \"%.1f seconds\" % (s,)\n",
        "    elif seconds < 4000:\n",
        "        m = seconds / 60.0\n",
        "        return \"%.2f minutes\" % (m,)\n",
        "    else:\n",
        "        h = seconds / 3600.0\n",
        "        return \"%.2f hours\" % (h,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "7W6tHgyyBzrr"
      },
      "outputs": [],
      "source": [
        "def build_model(maze, lr=0.001):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
        "    model.add(PReLU())\n",
        "    model.add(Dense(maze.size))\n",
        "    model.add(PReLU())\n",
        "    model.add(Dense(num_actions))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "wK9lCeTACAcy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading weights from file: model.h5\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Cannot assign value to variable ' dense_12/kernel:0': Shape mismatch.The variable shape (64, 64), and the assigned value shape (100, 100) are incompatible.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[39m=\u001b[39m build_model(maze)\n\u001b[1;32m----> 2\u001b[0m qtrain(model, maze, n_epoch\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, max_memory\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m\u001b[39m*\u001b[39;49mmaze\u001b[39m.\u001b[39;49msize, data_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, weights_file \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mmodel.h5\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
            "Cell \u001b[1;32mIn[24], line 14\u001b[0m, in \u001b[0;36mqtrain\u001b[1;34m(model, maze, **opt)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mif\u001b[39;00m weights_file:\n\u001b[0;32m     13\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mloading weights from file: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (weights_file,))\n\u001b[1;32m---> 14\u001b[0m     model\u001b[39m.\u001b[39;49mload_weights(weights_file)\n\u001b[0;32m     16\u001b[0m \u001b[39m# Construct environment/game from numpy array: maze (see above)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m qmaze \u001b[39m=\u001b[39m Qmaze(maze)\n",
            "File \u001b[1;32mc:\\Users\\Ploita\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32mc:\\Users\\Ploita\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\backend.py:4360\u001b[0m, in \u001b[0;36m_assign_value_to_variable\u001b[1;34m(variable, value)\u001b[0m\n\u001b[0;32m   4357\u001b[0m     variable\u001b[39m.\u001b[39massign(d_value)\n\u001b[0;32m   4358\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   4359\u001b[0m     \u001b[39m# For the normal tf.Variable assign\u001b[39;00m\n\u001b[1;32m-> 4360\u001b[0m     variable\u001b[39m.\u001b[39;49massign(value)\n",
            "\u001b[1;31mValueError\u001b[0m: Cannot assign value to variable ' dense_12/kernel:0': Shape mismatch.The variable shape (64, 64), and the assigned value shape (100, 100) are incompatible."
          ]
        }
      ],
      "source": [
        "model = build_model(maze)\n",
        "qtrain(model, maze, n_epoch=5, max_memory=8*maze.size, data_size=32) #, weights_file = 'model.h5')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ERXGZRE8rz7U"
      },
      "source": [
        "**Trajectory generated from a random point with the trained network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7w9ScypSsUhJ"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Exhibition of the maximum Q value at each state of the environment\n",
        "directions = []\n",
        "for rat_cell in qmaze.free_cells:\n",
        "    qmaze.reset(rat_cell)\n",
        "    envstate = qmaze.observe()\n",
        "    q = model.predict(envstate, verbose = 0)\n",
        "    action = np.argmax(q[0])\n",
        "    if action == 0:\n",
        "        directions.append('<b')\n",
        "    elif action == 1:\n",
        "        directions.append('^b')\n",
        "    elif action == 2:\n",
        "        directions.append('>b')\n",
        "    else:\n",
        "        directions.append('vb')\n",
        "\n",
        "rat_cell = (0,0)\n",
        "qmaze.reset(rat_cell)\n",
        "show(qmaze)\n",
        "k = 0\n",
        "for point in qmaze.free_cells:\n",
        "    plt.plot(point[1],point[0],directions[k])\n",
        "    k = k + 1"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
